---
title: "Project 1"
author: "Wesley Newcomb"
date: "2023-03-24"
output:
  html_document: default
  pdf_document: default
df_print: paged
---
---
title: "Project 1"
author: "Name: Wesley Newcomb \n Partner: Manuel Ricardo Vargas and Diva Medina Camp"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    toc: true
    toc_depth: 3
header-includes:
  - \usepackage{booktabs}
  - \usepackage{longtable}
  - \usepackage{array}
  - \usepackage{multirow}
  - \usepackage{wrapfig}
  - \usepackage{float}
  - \usepackage{colortbl}
  - \usepackage{pdflscape}
  - \usepackage{tabu}
  - \usepackage{threeparttable}
  - \usepackage{threeparttablex}
  - \usepackage[normalem]{ulem}
  - \usepackage{makecell}
  - \usepackage{xcolor}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
#### Load necessary packages ####
# * These packages are not necessary to complete the assignment and or only used 
#   to provide an example. 
packages <- c("knitr", "kableExtra", "magrittr", "readr", "geosphere", "dplyr")

install_me <- packages[!(packages %in% installed.packages()[, "Package"])]
if (length(install_me)) install.packages(install_me)

library(knitr)
library(kableExtra)
library(magrittr)
library(readr)
library(geosphere)
library(dplyr)
```

## Background
The World Health Organization has recently employed a new data science initiative, *CSIT-165*, that uses data science to characterize pandemic diseases. 
*CSIT-165* disseminates data driven analyses to global decision makers.

*CSIT-165* is a conglomerate comprised of two fabricated entities: *Global Health Union (GHU)* and *Private Diagnostic Laboratories (PDL)*. 
Your and your partner's role is to play a data scientist from one of these two entities.

## Data
> [2019 Novel Coronavirus COVID-19 (2019-nCoV) Data Repository by John Hopkins CSSE](https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_time_series)
Data for 2019 Novel Coronavirus is operated by the John Hopkins University Center for Systems Science and Engineering (JHU CSSE).
Data includes daily time series CSV summary tables, including confirmations, recoveries, and deaths. 
Country/region are countries/regions hat conform to World Health Organization (WHO).
Lat and Long refer to coordinates references for the user. 
Date fields are stored in MM/DD/YYYY format.

For this project, we will use global data sets for COVID-19 associated confirmations and deaths.

In order to download these files without cloning the whole repository, enter one of two commands in your terminal depending on your machine:

Windows
wget https://address.to.data/goes/here.csv
Mac
curl https://address.to.data/goes/here.csv -O

### Instructions
The World Health Organization has recently employed a new data science initiative, CSIT-165, that uses data science to characterize pandemic diseases. CSIT-165 disseminates data driven analyses to global decision makers.

CSIT-165 is a conglomerate comprised of two fabricated entities: Global Health Union (GHU) and Private Diagnostic Laboratories (PDL). Your and your partner’s role is to play a data scientist from one of these two entities. Discuss with your partner to decide who will be part of GHU and PDL.

### Getting Started
One project member per group must create a new repository on GitHub. Initialize this repository with a readme.md file that lists each member of the group. If your group decides to collaborate using a centralized workflow (recommended), then the project member that created the repository must declare their partners as collaborators in GitHub. Each project member will clone this repository onto their machine using RStudio. In RStudio, create a project from version control with GitHub using the HTTP address of the repository created by project member.

All project members must first contribute to analyses by uploading data sets respective to the entity they belong to in the CSIT-165 initiative. GHU project members provide time series data counting COVID-19 related recoveries and and deaths. PDL project members provide time series data counting COVID-19 related confirmations.



## Project Objectives
This project will encompass many of the lessons we have learned throughout the course. RMarkdown files must be written such that each time you render the document it will download the necessary data sets for analysis. Please render the RMarkdown file the day it is due to reflect the most recent data sets. With this added functionality, your code must be able to analyze the datasets regardless of the date you render your document.

### Objective 1
CSIT-165’s first objective is to determine where COVID-19 originated from. Predict where the origin started based on the area with the greatest number of confirmations and deaths on the first recorded day in the data set. Show this is the origin using an if statement.
```{r ob1}

confirmed_df<-read.csv("data/time_series_covid19_confirmed_global.csv", 
                       header=TRUE, stringsAsFactors=FALSE)
deaths_df<-read.csv("data/time_series_covid19_deaths_global.csv", 
                    header=TRUE, stringsAsFactors=FALSE)
recovered_df<-read.csv("data/time_series_covid19_recovered_global.csv", 
                       header=TRUE, stringsAsFactors=FALSE)

## Confirmed Dataset
confirmed_ordered<-select(arrange(confirmed_df, -X1.22.20), 
                          Province.State, Country.Region, X1.22.20)
cat("Confirmed Dataset ")
head(confirmed_ordered)

## Deaths Dataset
death_ordered<-select(arrange(deaths_df, -X1.22.20), 
                      Province.State, Country.Region, X1.22.20)
cat("Deaths Dataset ")
head(death_ordered)

## Recovered Dataset
recovered_ordered<-select(arrange(recovered_df, -X1.22.20), 
                          Province.State, Country.Region, X1.22.20)
cat("Recovered Dataset \n")
head(recovered_ordered)

cat(confirmed_ordered[1,1], ", ", confirmed_ordered[1,2], 
    " has the most confirmed cases on the first day. \n", 
    death_ordered[1,1], ", ", death_ordered[1,2], 
    " has the most deaths from the virus on the first day. \n",
    recovered_ordered[1,1], ", ", recovered_ordered[1,2], 
    " has the most recovered cases from the virus on the first day. \n", 
    sep="")


if (confirmed_ordered[1,1] == death_ordered[1,1] && 
    confirmed_ordered[1,1] == recovered_ordered[1,1] && 
    death_ordered[1,1] == recovered_ordered[1,1])
{
  cat(confirmed_ordered[1,1], ", ", confirmed_ordered[1,2], 
    " is the most likely origin of the virus. \n", sep="")
}
```
Based on the data above: Hubei, China is the most likely origin of the virus.  This is based on Hubei, China having by far the most confirmed cases and is the only region with any recoveries or deaths on the first day of data avaliable.  Also, all other regions that have confirmed cases on the first day are regions that are near Hubei. The conditional statement also proves that Hubei is the most likely origin of the virus because it shows that the place with the most deaths, recovered, and confirmed cases on the first day is Hubei.


### Objective 2
Where is the most recent area to have a first confirmed case? To do this, you will need to use a for loop, if statement, and subsets.
```{r ob2}
confirmed_df<-read.csv("data/time_series_covid19_confirmed_global.csv", 
                       header=TRUE, stringsAsFactors=FALSE)
recent_df<-arrange(confirmed_df[confirmed_df[,ncol(confirmed_df)-1] == 0 
                                & confirmed_df[,ncol(confirmed_df)] > 0,])

i<-0
# If there are no new cases today loop back to find most recent region
# to have new cases
if (nrow(recent_df) == 0) {
  while (nrow(recent_df) == 0) {
    i<-i+1
    recent_df<-arrange(confirmed_df[confirmed_df[,ncol(confirmed_df)-1-i] == 0 
                                & confirmed_df[,ncol(confirmed_df)-i] > 0,])
  }
}

head(select(recent_df, Province.State, Country.Region, ncol(confirmed_df)-1-i, 
            ncol(confirmed_df)-i))

# Vector is small enough that loop is reasonable
for(i in 1:nrow(recent_df))
{
  if (recent_df[i,1] == "") {
    cat(recent_df[i,2], "has recently had their first confirmed case. \n")
  } else {
    if (recent_df[i,2] == "") {
      cat(recent_df[i,1], "has recently had their first confirmed case. \n")
    } else {
      cat(recent_df[i,1], ", ", recent_df[i,2], 
          " has recently had their first confirmed case. \n", sep="")
    }
  }
}

```
Most recent territories were found by going through the data and selecting the countries that did not have cases before yesterday and had their first cases today.  If there are no regions meeting this check, then each previous day is looked at until there are regions found with new cases.


### Objective 3
How far away are the areas from objective 2 from where the first confirmed case(s) occurred? Please provide answer(s) in terms of miles. Use the function distm from the R package geosphere to calculate the distance between two coordinates in meters (geosphere::distm). You will need to convert the value returned by distm from meters to miles (this conversion is simple and can be found online). Please use a table or printed statement to describe what Province/State and Country/Region first confirmed cases occurred as well as the distance (in miles) away from the origin. Please print the following: {recent region} is {distance in miles} away from {origin city, origin country}.
```{r ob3}
origin_df<-arrange(confirmed_df, -X1.22.20)[1,]
confirmed_df<-read.csv("data/time_series_covid19_confirmed_global.csv", 
                       header=TRUE, stringsAsFactors=FALSE)
recent_df<-arrange(confirmed_df[confirmed_df[,ncol(confirmed_df)-1] == 0 
                                & confirmed_df[,ncol(confirmed_df)] > 0,])
i<-0

# If there are no new cases today loop back to find most recent region
# to have new cases
if (nrow(recent_df) == 0) {
  while (nrow(recent_df) == 0) {
    i<-i+1
    recent_df<-arrange(confirmed_df[confirmed_df[,ncol(confirmed_df)-1-i] == 0 
                                & confirmed_df[,ncol(confirmed_df)-i] > 0,])
  }
}

# Compute distances from origin
distances<-distm(select(recent_df, Long, Lat), select(origin_df, Long, Lat))

# Convert from m to miles
distances<-distances * 0.00062137

# Add distance from origin to dataframe and sort by distance
recent_df$distance<-distances[,1]
recent_df<-arrange(recent_df, distance)

head(select(recent_df, Province.State, Country.Region, Lat, Long, distance))

# Vector is small enough that loop is reasonable
for (i in 1:nrow(recent_df)) {
  city<-recent_df[i, "Province.State"]
  
  # If there is no city use country
  if (city == "") {
    city<-recent_df[i, "Country.Region"]
  }
  
  cat(city, "is", recent_df[i, "distance"], 
      "miles away from the virus origin in", 
      paste0(origin_df[1, "Province.State"], ","), 
      paste0(origin_df[1, "Country.Region"], "."), "\n")
}

```

### Objective 4
CSIT-165 characterizes diseases using risk scores. Risk scores are calculated as the ratio of deaths to confirmations, that is Riskscore=100×deathsconfirmations
. Risk scores equal to 100 indicate the highest risk while risk scores equal to 0 indicate the lowest risk. Areas are characterized as being especially vulnerable to loss if they have higher risk scores. For this assignment, exclude cruise ships (hint: they have lat and long coordinates of 0 or NA in this data set, filter this out before calculating risk scores).

Which area of the world currently has the lowest risk score (if more than one, display the one with the most confirmations)? Which area of the world currently has the highest risk score (if more than one, display the one with the most confirmations)? How do risk scores in these areas compare to global risk score? Why might it be helpful to calculate metrics like risk scores for different areas of the world and what would their limitations be (what assumptions does risk score make and what important variables might be left out)?

#### Objective 4.1
```{r ob4.1}
# Datasets respresent a cumlative sum by date, so last column represents 
# summation for region
confirmed_df<-read.csv("data/time_series_covid19_confirmed_global.csv", 
                       header=TRUE, stringsAsFactors=FALSE)
confirmed_df<-select(confirmed_df, Province.State, 
                     Country.Region, ncol(confirmed_df))
names(confirmed_df)[3] <- "confirmed"
deaths_df<-read.csv("data/time_series_covid19_deaths_global.csv", 
                    header=TRUE, stringsAsFactors=FALSE)
deaths_df<-select(deaths_df, Province.State, 
                  Country.Region, ncol(deaths_df))
names(deaths_df)[3] <- "deaths"
recovered_df<-read.csv("data/time_series_covid19_recovered_global.csv", 
                       header=TRUE, stringsAsFactors=FALSE)
recovered_df<-select(recovered_df, Province.State, 
                     Country.Region, ncol(recovered_df))
names(recovered_df)[3] <- "recovered"

# Combine the datasets into one and fill NA with 0
combined_df<-full_join(confirmed_df, recovered_df, 
                       by=c("Province.State", "Country.Region"))
combined_df<-full_join(combined_df, deaths_df, 
                       by=c("Province.State", "Country.Region"))
combined_df[is.na(combined_df)] <- 0

# Assignment is unclear if we are to consider state and region or 
# just region. Based on how data is formatted, I think it is cleaner 
# and makes more sence to use region only.  For instance, in confirmed 
# dataset, Canada is broken up by region, but in recovered dataset it 
# uses Canada as a whole.  There are numerous examples of this in
# the data
grouped_df<-as.data.frame(summarise_each(group_by(
  select(combined_df, -Province.State), Country.Region), sum))

# Compute risk and burden by region
grouped_df$risk<-grouped_df$deaths / grouped_df$recovered
grouped_df$burden<-grouped_df$confirmed * grouped_df$risk

cat("Highest risk scores ")
head(arrange(grouped_df, -risk, -confirmed))
cat("Highest risk scores, that are not infinite ")
head(arrange(grouped_df[grouped_df$risk != Inf,], -risk, -confirmed))
cat("Lowest Risk Scores ")
head(arrange(grouped_df, risk, confirmed))
cat("Lowest risk scores, that are not zero ")
head(arrange(grouped_df[grouped_df$risk != 0,], risk, confirmed))

global_confirmed<-sum(grouped_df$confirmed)
global_deaths<-sum(grouped_df$deaths)
global_recovered<-sum(grouped_df$recovered)
global_risk<-global_deaths / global_recovered
global_burden<-global_confirmed * global_risk

cat("Global Data\n", 
    "Confirmed:", global_confirmed, "\n", 
    "Deaths:   ", global_deaths, "\n",
    "Recovered:", global_recovered, "\n",
    "Risk:     ", global_risk, "\n",
    "Burden:   ", global_burden, "\n")

```
Based on how the equation is written, any region which has had at least one person recover and no deaths will have a risk score of zero.  Examples of this can be seen in the "Lowest Risk Scores" table above.  When filtering out risk scores of 0, the regions of lowest risk can be seen in "Lowest risk scores, that are not zero" table above.  Any region that has no recoveries and yet at least one death will have infinite risk.  Examples of this can be seen in the "Highest risk scores" table above.  If filtering out regions that have infinite risk, we see that the regions in the "Highest risk scores, that are not infinite" table above.  When looking at the global score, it seems like the risk is high when considering it represents the people that have recovered versus those who have died.  This value seems especially high when looking at the regions in the "Lowest risk scores, that are not zero" table, but when compared to the "Highest risk scores, that are not infinite" table where the risk scores are extremely high, the global risk seems less significant.  This wide range in risk numbers indicates that while the risk in some regions is extremely high, for the most part, the risk is rather low globally.

Risk assessments like this are important because they are good indicators of where danger is located or help is needed that can be used across many industries.  For example, the travel industry may wish to impose bans on travelling to and from locations of high risk.  The medical field can use these values to determine locations that are in the most need for medical support.  Research fields may also use this data to help identify trends.  For example, if a region has a high amount of recoveries and almost no deaths, i.e. low risk score, it may be worth looking into what kind of treatment they are using in that region and if it could be used in other locations throughout the world. The thing to be careful though is that risk scores may be a little misleading.  For instance, several regions have almost no cases, but one death and no recoveries causing a massive risk score.  Even though these regions have pretty much no cases, they are still seen as extremely risky.  This is why it could be beneficial to filter out the extremes before considering the data as valid.



#### Objective 4.2
You are asked to make two tables with the top 10 countries that have the most COVID-19 related confirmations and and deaths. Make sure to include all of the counts for the country, not just the counts for one area in the country. To do this we will need to sum all of the values for each country, create new data frames from these values, and use the package kable to convert those data frames into tables.

Hint: Sum each country’s counts by subsetting the data frame using a list of countries available in the data set. Use a for loop to iterate through the data frame using the list of countries. For each country, calculate the count sum and assign this value to a list.

```{r ob4.2}
# Datasets respresent a cumlative sum by date, so last column represents 
# sumation for region
confirmed_df<-read.csv("data/time_series_covid19_confirmed_global.csv", 
                       header=TRUE, stringsAsFactors=FALSE)
confirmed_df<-select(confirmed_df, Province.State, 
                     Country.Region, ncol(confirmed_df))
names(confirmed_df)[3] <- "confirmed"
deaths_df<-read.csv("data/time_series_covid19_deaths_global.csv", 
                    header=TRUE, stringsAsFactors=FALSE)
deaths_df<-select(deaths_df, Province.State, 
                  Country.Region, ncol(deaths_df))
names(deaths_df)[3] <- "deaths"
recovered_df<-read.csv("data/time_series_covid19_recovered_global.csv", 
                       header=TRUE, stringsAsFactors=FALSE)
recovered_df<-select(recovered_df, Province.State, 
                     Country.Region, ncol(recovered_df))
names(recovered_df)[3] <- "recovered"

# Combine the datasets into one and fill NA with 0
combined_df<-full_join(confirmed_df, recovered_df, 
                       by=c("Province.State", "Country.Region"))
combined_df<-full_join(combined_df, deaths_df, 
                       by=c("Province.State", "Country.Region"))
combined_df[is.na(combined_df)] <- 0

# Group and combine data by region
grouped_df<-as.data.frame(summarise_each(group_by(
  select(combined_df, -Province.State), Country.Region), sum))

# Compute risk and burden by region
grouped_df$risk<-grouped_df$deaths / grouped_df$recovered
grouped_df$burden<-grouped_df$confirmed * grouped_df$risk

confirmed_tb = kable(arrange(grouped_df, -confirmed)[1:5,])
deaths_tb = kable(arrange(grouped_df, -deaths)[1:5,])
recovered_tb = kable(arrange(grouped_df, -recovered)[1:5,])

cat("Top 5 confirmed regions ")
confirmed_tb
cat("Top 5 deaths regions ")
deaths_tb
cat("Top 5 recovered regions")
recovered_tb

```

### GitHub Log

#```{bash gitlog} 
#git log --pretty=format:"%nSubject: %s%nAuthor: %aN%nDate: %aD%nBody: %b"
#```
